<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Qingsen Yan</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Qingsen Yan</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 10 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Qingsen Yan</title>
      <link>/</link>
    </image>
    
    <item>
      <title>High dynamic range imaging via gradient-aware context aggregation network</title>
      <link>/publication/pr21/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      <guid>/publication/pr21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dual-Attention-Guided Network for Ghost-Free High Dynamic Range Imaging</title>
      <link>/publication/ijcv21/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/publication/ijcv21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Accurate HDR Imaging with Learning Generator Constraints</title>
      <link>/publication/neuro21/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/publication/neuro21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An attention-guided deep neural network with multi-scale feature fusion for liver vessel segmentation</title>
      <link>/publication/jbhi21/</link>
      <pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/jbhi21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ghost removal via channel attention in exposure fusion</title>
      <link>/publication/cviu/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/cviu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vessel Seg</title>
      <link>/project/jhbi/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/project/jhbi/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Liver vessel segmentation is fast becoming a key instrument in the diagnosis and surgical planning of liver diseases. In clinical practice, liver vessels are normally manual annotated by clinicians on each slice of CT images, which is extremely laborious. Several deep learning methods existed for liver vessel segmentation, however, promoting the performance of segmentation remains a major challenge due to the large variations and complex structure of liver vessels. Previous methods mainly using existing UNet architecture, but not all features of the encoder are useful for segmentation and some even cause interferences. To overcome this problem, we propose a novel deep neural network for liver vessel segmentation, called LVSNet, which employed special designs to obtain the accurate structure of the liver vessel. Specifically, we design Attention-Guided Concatenation (AGC) module to adaptively select the useful context features from low-level features guided by high-level features. The proposed AGC module focuses on capturing rich complemented information to obtain more details. In addition, we introduce an innovative multi-scale fusion block by constructing hierarchical residual-like connections within one single residual block, which is great importance for effectively linking the local blood vessel fragments together. Furthermore, we construct a new dataset containing 40 thin thickness cases (0.625mm) which consist of CT volumes and annotated vessels. To evaluate the effectiveness of the method with minor vessel, we also propose an automatic stratification method to split major and minor liver vessels. Extensive experimental results demonstrate that the proposed LVSNet outperforms previous methods on liver vessel segmentation datasets. Additionally, we conduct a series of ablation studies that comprehensively support the superiority of the underlying concepts.&lt;/p&gt;
&lt;h2 id=&#34;framework&#34;&gt;Framework&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-proposed-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The proposed method.&#34; srcset=&#34;
               /project/jhbi/frame_hu8e4290505f5c607e6eac24eb858b98fb_379798_c629d419211bf3c0331997d4dc18a7a2.png 400w,
               /project/jhbi/frame_hu8e4290505f5c607e6eac24eb858b98fb_379798_7341eadcf856d057e9f204a25ac3d781.png 760w,
               /project/jhbi/frame_hu8e4290505f5c607e6eac24eb858b98fb_379798_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;/project/jhbi/frame_hu8e4290505f5c607e6eac24eb858b98fb_379798_c629d419211bf3c0331997d4dc18a7a2.png&#34;
               width=&#34;760&#34;
               height=&#34;358&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The proposed method.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;examples-of-the-results&#34;&gt;Examples of the Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-results&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Results.&#34; srcset=&#34;
               /project/jhbi/exp1_hu84846b8ba7695ea7266a515380471fa4_2204506_2f63090c0fbaa811052d8a0c467e7595.png 400w,
               /project/jhbi/exp1_hu84846b8ba7695ea7266a515380471fa4_2204506_2cd58c8a5c86d90cbcab266c5848aafd.png 760w,
               /project/jhbi/exp1_hu84846b8ba7695ea7266a515380471fa4_2204506_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;/project/jhbi/exp1_hu84846b8ba7695ea7266a515380471fa4_2204506_2f63090c0fbaa811052d8a0c467e7595.png&#34;
               width=&#34;760&#34;
               height=&#34;593&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Results.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ‘‰ &lt;a href=&#34;https://github.com/qingsenyangit/Liver-vessel-Dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;See&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Attention-based network for low-light image enhancement</title>
      <link>/publication/icme/</link>
      <pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/publication/icme/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Blindly assess image quality in the wild guided by a self-adaptive hyper network</title>
      <link>/publication/cvpr20/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/publication/cvpr20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system</title>
      <link>/publication/covid19-sys/</link>
      <pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/publication/covid19-sys/</guid>
      <description></description>
    </item>
    
    <item>
      <title>COVID-19 Chest CT Image Segmentation Network by Multi-Scale Fusion and Enhancement Operations</title>
      <link>/publication/covid19-method/</link>
      <pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/publication/covid19-method/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep HDR Imaging via A Non-local Network</title>
      <link>/publication/tip20/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/publication/tip20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>HDR Deghosting</title>
      <link>/project/ahdr/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/project/ahdr/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Ghosting artifacts caused by moving objects or misalignments is a key challenge in high dynamic range (HDR) imaging for dynamic scenes. Previous methods first register the input low dynamic range (LDR) images using optical flow before merging them, which are error-prone and cause ghosts in results. A very recent work tries to bypass optical flows via a deep network with skip-connections, however, which still suffers from ghosting artifacts for severe movement. To avoid the ghosting from the source, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) to produce high-quality ghost-free HDR images. Unlike previous methods directly stacking the LDR images or features for merging, we use attention modules to guide the merging according to the reference image. The attention modules automatically suppress undesired components caused by misalignments and saturation and enhance desirable fine details in the non-reference images. In addition to the attention model, we use dilated residual dense block (DRDB) to make full use of the hierarchical features and increase the receptive field for hallucinating the missing details. The proposed AHDRNet is a non-flow-based method, which can also avoid the artifacts generated by optical-flow estimation error. Experiments on different datasets show that the proposed AHDRNet can achieve state-of-the-art quantitative and qualitative results.&lt;/p&gt;
&lt;h2 id=&#34;framework&#34;&gt;Framework&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-proposed-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The proposed method.&#34; srcset=&#34;
               /project/ahdr/frame_hu5e8cd8f6e148936bfa0ad5c38c4f0de7_1642022_2d38363aa73328a255ea8d3d71462a72.png 400w,
               /project/ahdr/frame_hu5e8cd8f6e148936bfa0ad5c38c4f0de7_1642022_0a3b09030cda569275e393c35c45e848.png 760w,
               /project/ahdr/frame_hu5e8cd8f6e148936bfa0ad5c38c4f0de7_1642022_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;/project/ahdr/frame_hu5e8cd8f6e148936bfa0ad5c38c4f0de7_1642022_2d38363aa73328a255ea8d3d71462a72.png&#34;
               width=&#34;760&#34;
               height=&#34;206&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The proposed method.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;examples-of-the-results&#34;&gt;Examples of the Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-proposed-method-can-remove-the-ghosting-artifacts&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The proposed method can remove the ghosting artifacts.&#34; srcset=&#34;
               /project/ahdr/fig7_hue469f6dea0857a4bd0a2a405ce591e16_3759173_f39a4729b09d7758787462fa7478a5e5.jpg 400w,
               /project/ahdr/fig7_hue469f6dea0857a4bd0a2a405ce591e16_3759173_86da09b329fe2d9d798dd914a6bb0032.jpg 760w,
               /project/ahdr/fig7_hue469f6dea0857a4bd0a2a405ce591e16_3759173_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;/project/ahdr/fig7_hue469f6dea0857a4bd0a2a405ce591e16_3759173_f39a4729b09d7758787462fa7478a5e5.jpg&#34;
               width=&#34;760&#34;
               height=&#34;502&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The proposed method can remove the ghosting artifacts.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-this-sample-shows-the-ahdrnet-can-handle-over-saturation-regions&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;This sample shows the AHDRNet can handle over-saturation regions.&#34; srcset=&#34;
               /project/ahdr/fig8_hu9d46132c1a717920148b945e50839526_1933687_81067cb3b85cef650de8f48a6c8d01c8.jpg 400w,
               /project/ahdr/fig8_hu9d46132c1a717920148b945e50839526_1933687_3b3c37dd2c0395ba74cff6ef669053ef.jpg 760w,
               /project/ahdr/fig8_hu9d46132c1a717920148b945e50839526_1933687_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;/project/ahdr/fig8_hu9d46132c1a717920148b945e50839526_1933687_81067cb3b85cef650de8f48a6c8d01c8.jpg&#34;
               width=&#34;760&#34;
               height=&#34;510&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      This sample shows the AHDRNet can handle over-saturation regions.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ‘‰ &lt;a href=&#34;https://github.com/qingsenyangit/AHDRNet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;See&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Attention-guided network for ghost-free high dynamic range imaging</title>
      <link>/publication/cvpr19/</link>
      <pubDate>Tue, 25 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/publication/cvpr19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-scale dense networks for deep high dynamic range imaging</title>
      <link>/publication/wacv/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/publication/wacv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two-stream convolutional networks for blind image quality assessment</title>
      <link>/publication/tip18/</link>
      <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/publication/tip18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High dynamic range imaging by sparse representation</title>
      <link>/publication/neuro17/</link>
      <pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/publication/neuro17/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
