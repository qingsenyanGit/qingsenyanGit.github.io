[{"authors":null,"categories":null,"content":"Qingsen Yan is currently a Postdoctoral Researcher at the University of Adelaide, working with Prof. Qinfeng (Javen) Shi. He received a PhD in Computer Science from Northwestern Polytechnical University (Xiâ€™an, China) under the supervision of Prof. Yanning Zhang. During Jan. 2018 to Jan. 2019, he was a visiting scholar at Australian Centre for Visual Technologies, University of Adelaide, under the supervision of Prof. Ian Reid and Prof. Qinfeng (Javen) Shi.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Qingsen Yan is currently a Postdoctoral Researcher at the University of Adelaide, working with Prof. Qinfeng (Javen) Shi. He received a PhD in Computer Science from Northwestern Polytechnical University (Xiâ€™an, China) under the supervision of Prof.","tags":null,"title":"Qingsen Yan","type":"authors"},{"authors":["Qingsen Yan, Dong Gong, Javen Qinfeng Shi, Anton van den Hengel, Jinqiu Sun, Yu Zhu, Yanning Zhang"],"categories":null,"content":"","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"8b93e45714f8414d3c7198d2044eae36","permalink":"/publication/pr21/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/publication/pr21/","section":"publication","summary":"In Pattern Recognition, 2021","tags":[],"title":"High dynamic range imaging via gradient-aware context aggregation network","type":"publication"},{"authors":["Qingsen Yan, Dong Gong, Javen Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang"],"categories":null,"content":"","date":1635379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635379200,"objectID":"75280ef42a485f992062d19763158607","permalink":"/publication/ijcv21/","publishdate":"2020-12-25T00:00:00Z","relpermalink":"/publication/ijcv21/","section":"publication","summary":"In International Journal of Computer Vision 2021","tags":[],"title":"Dual-Attention-Guided Network for Ghost-Free High Dynamic Range Imaging","type":"publication"},{"authors":["Qingsen Yan, Bo Wang, Lei Zhang, Jingyu Zhang, Zheng You, Qinfeng Shi, Yanning Zhang"],"categories":null,"content":"","date":1616630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616630400,"objectID":"c96b79cb2c5cb0039dc328750e165349","permalink":"/publication/neuro21/","publishdate":"2021-03-25T00:00:00Z","relpermalink":"/publication/neuro21/","section":"publication","summary":"In Neurocomputing 2021","tags":[],"title":"Towards Accurate HDR Imaging with Learning Generator Constraints","type":"publication"},{"authors":["Qingsen Yan, Bo Wang, Wei Zhang, Chuan Luo, Wei Xu, Zhengqing Xu, Yanning Zhang, Qinfeng Shi, Liang Zhang, Zheng You"],"categories":null,"content":"","date":1608854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608854400,"objectID":"9a89fde8ed131d822f16992698bee353","permalink":"/publication/jbhi21/","publishdate":"2020-12-25T00:00:00Z","relpermalink":"/publication/jbhi21/","section":"publication","summary":"In IEEE Journal of Biomedical and Health Informatics 2020","tags":[],"title":"An attention-guided deep neural network with multi-scale feature fusion for liver vessel segmentation","type":"publication"},{"authors":["Qingsen Yan, Bo Wang, Peipei Li, Xianjun Li, Ao Zhang, Qinfeng Shi, Zheng You, Yu Zhu, Jinqiu Sun, Yanning Zhang"],"categories":null,"content":"","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"1b19fee04e62e79ef83ebd38c5c5427a","permalink":"/publication/cviu/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/publication/cviu/","section":"publication","summary":"In Computer Vision and Image Understanding, 2020","tags":[],"title":"Ghost removal via channel attention in exposure fusion","type":"publication"},{"authors":null,"categories":null,"content":"Abstract Liver vessel segmentation is fast becoming a key instrument in the diagnosis and surgical planning of liver diseases. In clinical practice, liver vessels are normally manual annotated by clinicians on each slice of CT images, which is extremely laborious. Several deep learning methods existed for liver vessel segmentation, however, promoting the performance of segmentation remains a major challenge due to the large variations and complex structure of liver vessels. Previous methods mainly using existing UNet architecture, but not all features of the encoder are useful for segmentation and some even cause interferences. To overcome this problem, we propose a novel deep neural network for liver vessel segmentation, called LVSNet, which employed special designs to obtain the accurate structure of the liver vessel. Specifically, we design Attention-Guided Concatenation (AGC) module to adaptively select the useful context features from low-level features guided by high-level features. The proposed AGC module focuses on capturing rich complemented information to obtain more details. In addition, we introduce an innovative multi-scale fusion block by constructing hierarchical residual-like connections within one single residual block, which is great importance for effectively linking the local blood vessel fragments together. Furthermore, we construct a new dataset containing 40 thin thickness cases (0.625mm) which consist of CT volumes and annotated vessels. To evaluate the effectiveness of the method with minor vessel, we also propose an automatic stratification method to split major and minor liver vessels. Extensive experimental results demonstrate that the proposed LVSNet outperforms previous methods on liver vessel segmentation datasets. Additionally, we conduct a series of ablation studies that comprehensively support the superiority of the underlying concepts.\nFramework   The proposed method.  Examples of the Results   Results.  Code  ðŸ‘‰ See  ","date":1595808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595808000,"objectID":"fe966f8819df7013d94b2aa03ce9da22","permalink":"/project/jhbi/","publishdate":"2020-07-27T00:00:00Z","relpermalink":"/project/jhbi/","section":"project","summary":"JBHI20","tags":["Medical Image"],"title":"Vessel Seg","type":"project"},{"authors":["Cheng Zhang*, Qingsen Yan*, Yu Zhu, Xianjun Li, Jinqiu Sun, Yanning Zhang (* Equal Contribution)"],"categories":null,"content":"","date":1595635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595635200,"objectID":"eb37639041c5883883dfcb7f56c29fda","permalink":"/publication/icme/","publishdate":"2020-07-25T00:00:00Z","relpermalink":"/publication/icme/","section":"publication","summary":"IEEE International Conference on Multimedia and Expo, 2020","tags":[],"title":"Attention-based network for low-light image enhancement","type":"publication"},{"authors":["Shaolin Su*, Qingsen Yan*, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, Yanning Zhang (* Equal Contribution)"],"categories":null,"content":"","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"5ed691ef5d0e4bf5d482cf0d3e190670","permalink":"/publication/cvpr20/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/publication/cvpr20/","section":"publication","summary":"In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020","tags":[],"title":"Blindly assess image quality in the wild guided by a self-adaptive hyper network","type":"publication"},{"authors":["Bo Wang, Shuo Jin, Qingsen Yan, Haibo Xu, Chuan Luo, Lai Wei, Wei Zhao, Xuexue Hou, Wenshuo Ma, Zhengqing Xu, Zhuozhao Zheng, Wenbo Sun, Lan Lan, Wei Zhang, Xiangdong Mu, Chenxin Shi, Zhongxiao Wang, Jihae Lee, Zijian Jin, Minggui Lin, Hongbo Jin, Liang Zhang, Jun Guo, Benqi Zhao, Zhizhong Ren, Shuhao Wang, Wei Xu, Xinghuan Wang, Jianming Wang, Zheng You, Jiahong Dong"],"categories":null,"content":"","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"f6a476977d48d9f22e51db82d6fd77e4","permalink":"/publication/covid19-sys/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/publication/covid19-sys/","section":"publication","summary":"Applied Soft Computing, 2020 (Highly Cited Papers)","tags":[],"title":"AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system","type":"publication"},{"authors":["Qingsen Yan, Bo Wang, Dong Gong, Chuan Luo, Wei Zhao, Jianhu Shen, Jingyang Ai, Qinfeng Shi, Yanning Zhang, Shuo Jin, Liang Zhang, Zheng You"],"categories":null,"content":"","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"a802ffa408973db8b8175cba536a1dc3","permalink":"/publication/covid19-method/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/publication/covid19-method/","section":"publication","summary":"IEEE Transactions on Big Data, 2021","tags":[],"title":"COVID-19 Chest CT Image Segmentation Network by Multi-Scale Fusion and Enhancement Operations","type":"publication"},{"authors":["Qingsen Yan, Lei Zhang, Yu Liu, Yu Zhu, Jinqiu Sun, Qinfeng Shi, Yanning Zhang"],"categories":null,"content":"","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"ee31c148eb6af21d4c4079d5003d37e3","permalink":"/publication/tip20/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/publication/tip20/","section":"publication","summary":"In IEEE Transactions on Image Processing, 2020","tags":[],"title":"Deep HDR Imaging via A Non-local Network","type":"publication"},{"authors":null,"categories":null,"content":"Abstract Ghosting artifacts caused by moving objects or misalignments is a key challenge in high dynamic range (HDR) imaging for dynamic scenes. Previous methods first register the input low dynamic range (LDR) images using optical flow before merging them, which are error-prone and cause ghosts in results. A very recent work tries to bypass optical flows via a deep network with skip-connections, however, which still suffers from ghosting artifacts for severe movement. To avoid the ghosting from the source, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) to produce high-quality ghost-free HDR images. Unlike previous methods directly stacking the LDR images or features for merging, we use attention modules to guide the merging according to the reference image. The attention modules automatically suppress undesired components caused by misalignments and saturation and enhance desirable fine details in the non-reference images. In addition to the attention model, we use dilated residual dense block (DRDB) to make full use of the hierarchical features and increase the receptive field for hallucinating the missing details. The proposed AHDRNet is a non-flow-based method, which can also avoid the artifacts generated by optical-flow estimation error. Experiments on different datasets show that the proposed AHDRNet can achieve state-of-the-art quantitative and qualitative results.\nFramework   The proposed method.  Examples of the Results   The proposed method can remove the ghosting artifacts.    This sample shows the AHDRNet can handle over-saturation regions.  Code  ðŸ‘‰ See  ","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"ed64cf97e2454a034773d4e06c89322a","permalink":"/project/ahdr/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/project/ahdr/","section":"project","summary":"CVPR19","tags":["HDR Image"],"title":"HDR Deghosting","type":"project"},{"authors":["Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang"],"categories":null,"content":"","date":1561420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561420800,"objectID":"e6b3a7dcc6c37e88379e550a2203c276","permalink":"/publication/cvpr19/","publishdate":"2019-06-25T00:00:00Z","relpermalink":"/publication/cvpr19/","section":"publication","summary":"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019","tags":[],"title":"Attention-guided network for ghost-free high dynamic range imaging","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Qingsen Yan, Dong Gong, Pingping Zhang, Qinfeng Shi, Jinqiu Sun, Ian Reid, Yanning Zhang"],"categories":null,"content":"","date":1548374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548374400,"objectID":"a2762254bed5157e32482c573b361896","permalink":"/publication/wacv/","publishdate":"2019-01-25T00:00:00Z","relpermalink":"/publication/wacv/","section":"publication","summary":"IEEE Winter Conference on Applications of Computer Vision, 2019","tags":[],"title":"Multi-scale dense networks for deep high dynamic range imaging","type":"publication"},{"authors":["Qingsen Yan, Dong Gong, Yanning Zhang"],"categories":null,"content":"","date":1543104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543104000,"objectID":"f3fd4522f7effeeacfaa42a2660e9b6b","permalink":"/publication/tip18/","publishdate":"2018-11-25T00:00:00Z","relpermalink":"/publication/tip18/","section":"publication","summary":"IEEE Transactions on Image Processing, 2018","tags":[],"title":"Two-stream convolutional networks for blind image quality assessment","type":"publication"},{"authors":["Qingsen Yan, Jinqiu Sun, Haisen Li, Yu Zhu, Yanning Zhang"],"categories":null,"content":"","date":1514160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514160000,"objectID":"1e74dc76fe11e702fc30986ceffa3bde","permalink":"/publication/neuro17/","publishdate":"2017-12-25T00:00:00Z","relpermalink":"/publication/neuro17/","section":"publication","summary":"In Neurocomputing 2017","tags":[],"title":"High dynamic range imaging by sparse representation","type":"publication"}]